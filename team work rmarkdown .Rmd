---
title: "R package for GIST and GIST-Screening"
author: Zhou Ya, Ding Fei, Chen Yingyi
date: 2018.1.6
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The Origin
Here we present the codes derived from "Group Iterative Spectrum Thresholding for
Super-Resolution Sparse Spectral Selection".

Recently, sparsity-based algorithms are proposed for super-resolution spectrum estimation. However, to achieve adequately high resolution in real-world signal analysis, the dictionary atoms have to be close to each other in frequency, thereby resulting in a coherent design. The popular convex compressed sensing methods break down in presence of high coherence and large noise.

In the corresponding paper, the authers investigate super-resolution spectral recovery
from a statistical perspective and propose a group iterative spectrum thresholding (GIST) framework to tackle challenges.

GIST allows for (possibly nonconvex) shrinkage estimation and can exploit the pairing
structure.

They find that neither the $l_1$ nor the $l_0$ regularization is satisfactory for spectrum estimation, and advocate a hybrid $l_0+l_2$ type shrinkage estimation. 

Furthermore, a GIST variant provides a screening technique for supervised dimension
reduction to deal with applications in ultrahigh dimensions.


## Model Setup
We introduce the problem of super-resolution spectrum estimation and review some existing methods from a statistical point of view.

- Let $y=[y(t_n)]_{1\le n\le N}$ be a real -valued signal contaminated with i.i.d. Gaussian noise $N(0, \sigma^2)$.

- The sampling time sequence $\{t_n\}_{1\le n\le N}$ is not required to be uniform.

- We use a grid of evenly spaced frequencies $f_k=f_{max}\cdot k/D$ for $k=0,1,\ldots, D$ to construct the sine and cosine frequency predictors,i.e. $\cos(2\pi t f_k)$ and $\sin(2\pi t f_k)$.

- Let $\mathcal{F}$ denote the set of nonzero frequencies $\{f_1,\ldots, f_D\}$.

- The upper band limit $f_{\max}$ can be $(2\min_{1\le n\le N}(t_n-t_{n-1})^{-1}$ or estimated based on the spectral window.

- The signal can be represented by 
$$\begin{align*}
y_n = y(t_n) = \sum_{k=0}^D A_k\cos(2\pi f_k t_n+\phi_k)+e_n,\ 1\le n\le N
\end{align*}
$$
where $A_k$, $\phi_k$ are unknown, and the noise $\{e_n\}_{n=1}^N$ are i.i.d. Gaussian with zero mean and unknown variance $\sigma^2$.

- From $$\begin{align*}
A_k\cos(2\pi f_k t_n+\phi_k)&=A_k\cos(\phi_k)\cos(2\pi f_k t_n)\\
&-A_k\sin(\phi_k)\sin(2\pi f_k t_n)\\
&=a_k\cos(2\pi f_k t_n)+b_k\sin(2\pi f_k t_n)
\end{align*}
$$
with $a_k=A_k\cos\phi_k$, $b_k=-A_k\sin\phi_k$, we introduce two column vectors
$$\begin{align*}
&X^{\cos}(f)\triangleq[\cos(2\pi t_n f)]_{1\le n\le N},\\
&X^{\sin}(f)\triangleq[\sin(2\pi t_n f)]_{1\le n\le N},
\end{align*}
$$

- Define the predictor matrix
$$\begin{align*}
X\triangleq[X^{\cos}(f_1),\ldots,X^{\cos}(f_D),X^{\sin}(f_1),\ldots,X^{\sin}(f_D)].
\end{align*}
$$

- Denote the coefficient vector by $\beta\in\mathbb{R}^{2D}$ and the intercept (zero frequency component) by $\alpha$.

- The model can be formulated as a linear regression
$$\begin{align*}
y=\alpha+X\beta+e
\end{align*}
$$
where $\beta$ is sparse and $e\sim N(0, \sigma^2 I)$.

- The concrete design matrix (without the intercept) is given by
$$
\begin{align*}
X=\begin{bmatrix} 
\cos(\pi\frac{1}{D}t_1)&\cdots&\cos(\pi\frac{D}{D}t_1)&\sin(\pi\frac{1}{D}t_1)\cdots\sin(\pi\frac{D}{D}t_1)\\
\vdots&\vdots&\vdots&\vdots\\
\cos(\pi\frac{1}{D}t_N)&\cdots&\cos(\pi\frac{D}{D}t_N)&\sin(\pi\frac{1}{D}t_N)\cdots\sin(\pi\frac{D}{D}t_N)\\
\end{bmatrix}
\end{align*}
$$


## GIST Framwork

- We study a group penalized least-squares model and investigate the appropriate type of regularization
- When the signal is corrupted by noise as in (3), the following $l_1$-penalized linear model is more commonly used:
$$
\begin{align*}
\frac{1}{2}\Vert y-\alpha-X\beta\Vert_2^2+\lambda\Vert\beta\Vert_1,
\end{align*}
$$
where $\lambda$ is a regularization parameter to provide a trade-off between the fitting error and solution sparsity.

- To include more sparsity-enforcing penalties, we consider a more
general problem in this paper which minimizes
$$
\begin{align*}
\frac{1}{2}\Vert y-\alpha-X\beta\Vert_2^2+\sum_{k=1}^{2D}P(|\beta_k|;\lambda)=:F(\beta;\lambda),
\end{align*}
$$
where $P(\cdot;\lambda)$ is a univariate penalty function parameterized by $\lambda$ and is possibly nonconvex.
- Some structural information can be further incorporated in spectrum estimation.

- $A_k=0$ implies $\beta_k=\beta_{D+k}=0$, i.e. the sine and cosine predictors at $f_k$ vanish simultaneously.

- The pairing structure shows it is more reasonable to impose the so-called group sparsity or block sparsity on $\{(\beta_k,\beta_{D+k})\}_{1\le k\le D}$ rather than the unstructured sparsity on $\{\beta_k\}_{1\le k\le 2D}$.

- The group penalizedmodel with the model design minimizes
$$
\begin{align*}
\frac{1}{2}\Vert y-\alpha-X\beta\Vert_2^2+\sum_{k=1}^{D}P(\sqrt{\beta_k^2+\beta_{D+k}^2};\lambda)=:F(\beta;\lambda),
\end{align*}
$$

- The popular $l_1$-penalty $P_1(t;\lambda)=\lambda|t|$ may result in insufficient sparsity and relatively large prediction error.

- There is still much room for improvement in super-resolution spectral estimation.

- It is worth pointing out that there are two objectives involved in this task：

- Objective 1 (O1): accurate prediction of the signal at any new time point in the time domain;

- Objective 2 (O2): parsimonious spectral representation of the signal in the Fourier domain.

- O1 + O2 complies with Occam’s razor principle—the simplest way to explain the data is the best.


## A Novel Regularization Form
- From the perspective of O2, the $l_0$-norm constructs an ideal penalty
$$
\begin{align*}
P_0(t;\lambda)=\frac{\lambda^2}{2}1_{t\neq 0},
\end{align*}
$$

- Given any model matrix, the class of penalties $aP_H(t;\lambda/\sqrt{a})$ for any $a\ge 1$ mimics the behavior, where $P_H$, referred to as the hard-penlaty, is defined by
$$
P_H(t;\lambda)=\Big\{
\begin{align*}
&-t^2/2+\lambda|t|,\quad\  if |t|<\lambda\\
&\lambda^2/2,\quad\quad\quad\quad if |t|\ge\lambda
\end{align*}
$$


- Tikhonov regularization is an effective means to deal with the singularity issue which seriously affects estimation and prediction accuracy. It is in the form of an $l_2$-norm penalty
$$
\begin{align*}
P_R(t;\eta)=\frac{1}{2}\eta t^2,
\end{align*}
$$
also known as the ridge penalty in statistics.

- Taking into account both concerns,we advocate the following hybrid hard-ridge (HR) penalty as a fusion result:
$$
P_{HR}(t;\lambda,\eta)=\Bigg\{
\begin{align*}
&-\frac{1}{2}t^2+\lambda|t|,\quad\quad\quad  if |t|<\frac{\lambda}{1+\eta}\\
&\frac{1}{2}\eta t^2+\frac{1}{2}\frac{\lambda^2}{1+\eta},\ \quad if |t|\ge\frac{\lambda}{1+\eta}
\end{align*}
$$


- Be attention to the $l_1+l_2$ penalty, i.e. $\lambda_1\Vert\beta\Vert_1+\lambda_2^2\Vert\beta\Vert_2^2/2$, may over-shrink the model (referred to as the double shrinkage effect) and cannot enforce higher level of sparsity than the $l_1$-penalty.

- In contrast, using a $q$-function trick, it is shown that $P_{HR}$ results in the same estimator as the $'l_0+l_2'$ penalty
$$
\begin{align*}
P(t;\lambda,\eta)=\frac{1}{2}\frac{\lambda^2}{1+\eta}1_{t\neq 0}+\frac{1}{2}\eta t^2
\end{align*}
$$

- The ridge part does not affect the nondifferential behavior of the $l_0$-norm at zero, and there is no double-shrinkage effect for nonzero coefficient estimates.

- We assume both $X$ and $y$ have been centered so that the intercept term vanishes in the model. Our main tool to tackle the computational challenge is the class of
$\Theta$-estimators

- the computational challenge is the class of $\Theta$-estimators. Let $\Theta(\cdot;\lambda)$ be an arbitrarily given threshold function (with as the parameter) which is odd, monotone, and a unbounded shrinkage rule with $\lambda$ as the parameter.

- A group $\Theta$-estimator is defined to be a solution to 
$$
\begin{align*}
\beta=\vec{\Theta}(\beta+X^T(y-X\beta);\lambda)
\end{align*}
$$

- A $\Theta$-estimator is necessarily a $P$-penalized estimator provided that
$$P(t;\lambda)-P(0;\lambda)=
\begin{align*}
\int_0^{|t|}(\sup\{s:\Theta(s;\lambda)\le u\}-u)du+q(t;\lambda)
\end{align*}
$$
holds for some nonnegative $q(\cdot;\lambda)$ satisfying $q(\Theta(s;\lambda)\lambda)=0$ for any $s\in\mathbb{R}$.

- If we define to be the hard-ridge thresholding:
$$
\Theta_{HR}(t;\lambda,\eta)=\Bigg\{
\begin{align*}
&0,\quad\quad\quad  if |t|<\lambda\\
&\frac{t}{1+\eta},\ \quad if |t|\ge\lambda
\end{align*}
$$
Then $P_{\Theta_{HR}}$ is the hard-ridge penalty.


## Probabilistic Spectra Screening
- Computational complexity is another major challenge in super-resolution studies. In GIST algorithm, each iteration step involves only matrix-vector multiplications and componentwise thresholding operations. 

- Both have low complexity and can be vectorized. 

- The total number of flops is no more than $(4DN+8D)\Omega$, which is linear in $D$.

- In our experiments, $\Omega=200$ suffices and thus the complexity of GIST algorithm is $O(DN)$.

- We can use the Fast Fourier transform (FFT) in computation to reduce the complexity to $O(D\log D)$.

- One may reduce the dimension from $2D$ to $\vartheta N$ (say $\vartheta=0.5$).

- If the $\vartheta N$ candidate predictors are wisely chosen, the truly relevant atoms will be included with high probability and the performance sacrifice in selection/estimation will be mild.

- Hereinafter, we call $\vartheta$ the candidate ratio.


## Simulation Setup
- Consider a discrete real-valued signal given by
$$
\begin{align*}
y(t_n)=\sum_{f_k\in nz^*}A_k\cos(2\pi f_k t_n+\phi_k)+e(t_n)
\end{align*}
$$
where $e(t_n)$ is white Gaussian noise with variance $\sigma^2$.

- $N=100$ training samples are observed at time $t_n=n,\ q\le n\le N$.

- Setting themaximum frequency $f_{\max}=0.5Hz$.

- resolution level $\delta=0.02Hz$.

- The number of frequency bins $D=f_{\max}/\delta=250$.

- Consider a discrete real-valued signal given by
$$
\begin{align*}
y(t_n)=\sum_{f_k\in nz^*}A_k\cos(2\pi f_k t_n+\phi_k)+e(t_n)
\end{align*}
$$

- Set $nz^*=\{0.248, 0.25, 0.252, 0.398, 0.4\}$

- The associated amplitude $A_k$ and phases $\phi_k$ given by $[2,4,3,3.5,3]$ and $[\pi/4, \pi/6, \pi/3, \pi/5, \pi/2]$, respectively.

- We vary the noise level by $\sigma^2=1, 4, 8$ to study the algorithmic performance. 

- Consider a discrete real-valued signal given by
$$
\begin{align*}
y(t_n)=\sum_{f_k\in nz^*}A_k\cos(2\pi f_k t_n+\phi_k)+e(t_n)
\end{align*}
$$

- Due to random fluctuation, reporting frequency identification for one particular simulation dataset is meaningless. Instead, we simulated each model $50$ times to enhance stability, where at each run $e(t_n)$ are i.i.d. following $N(0,\sigma^2)$.


##Here we install our PACKAGE
```{r}
library(FGIST)
```

## Code for Data Simulation
```{r}
set.seed(100)
sigma<-1
N<-100
y<-c()
for(i in 1:100){
  s<-0
  nz<-c(0.248,0.25,0.252,0.398,0.4)
  Ak<-c(2,4,3,3.5,3)
  phik<-c(pi/4,pi/6,pi/3,pi/5,pi/2)
  tn<-i
  m=1;
  for (fk in nz)
  {
    en<-rnorm(1,0,sigma)
    s=s+Ak[m]*cos(2*pi*fk*tn+phik[m])+en
    m = m+1
  }
  y[i]<-s
}

x<-c()
for(i in 1:250){
  for(j in 1:100){
    x[j+(i-1)*100]<-cos(pi*i/250*j)
  }
}

for(i in 1:250){
  for(j in 1:100){
    x[25000+j+(i-1)*100]<-sin(pi*i/250*j)
  }
}

X<-matrix(x,100,500)

```

## Normalize the data X and Center the data y,so we get the using Data X1 and y1
```{r}
X1<-matrix(,100,500)
for(i in 1:500){
  X1[,i]=(X[,i]-mean(X[,i]))/sqrt(var(X[,i]))
}

y1 = vector()
for(i in 1:length(y)){
  y1[i]= y[i]-1/length(y)*sum(y)
}
```

##The Results of the GroupIST 
```{r}
library(FGIST)
ResultGroupIST =GroupIST(X1,y1,lambda = 0.3,Omega = 10000,w = 0.5, epsilon = 0.01,eta =0.1,tau0=30)
ResultGroupIST$iteration
sum(ResultGroupIST$betahat!=0)
ResultGroupIST$`object function`[20]
```
The iteration number in this situation is 20, which means that the value of the objective function converges after 20 times. 

The current stable value of the objective function is 13.23321.

There are 18 parameters here which are not zero, then we come to the result that we have successfully chosen 18 parameters.


##The Results of the FastGroupIST 
```{r}
ResultFastGroupIST =FastGroupIST(X1,y1,Omega = 10000,w = 0.5, epsilon = 0.01,eta =0.1,tau0=30,theta=0.5)
ResultFastGroupIST$iteration
ResultFastGroupIST$`object function`[16]
sum(ResultFastGroupIST$betahat!=0)
```
The iteration number in this situation is 16, which means that the value of the objective function converges after 16 times. 

The current stable value of the objective function is 9.193812.

There are 102 parameters here which are not zero, then we come to the result that we have successfully chosen 102 parameters.


## Plot of GroupIST and FastGroupIST
```{r}
ite1=ResultGroupIST$iteration
ite2=ResultFastGroupIST$iteration
ite=max(ite2,ite1)
f1=ResultGroupIST$`object function`[1:ite]
f2=ResultFastGroupIST$`object function`[1:ite2]
beta1=ResultGroupIST$betahat
beta2=ResultFastGroupIST$betahat

#plot the values of object functions and numbers of iteration
plot(1:ite2,f2,xlab="iteration",
     ylab="value of object function",
     main="FastGroupIST VS GroupIST  ",
     type="s",col=2)
lines(1:ite,f1,col=1)
     legend("topright",inset=0.05,
     c("FastGroupIST","GroupIST"),
     lty=c(1,1),col=c("red","black"))
```
It can be apparently seen from the first plot that FastGroupIST converges faster than the GroupISt.

## plot the betahat 
```{r}          
plot(1:500,beta2,xlab="iteration",
          ylab="value of object function",
          main="FastGroupIST VS GroupIST  ",
          type="s",col=2)
lines(1:500,beta1,col=1)
legend("topright",inset=0.05,
       c("FastGroupIST","GroupIST"),
       lty=c(1,1),col=c("red","black"))
```
However, from the perspective of the second plot, we find that the effect of shrinkage of GroupIST is much better than the FastGroupIST. 

## A brief conclusion
Here in the plots presented above, it can be apparently seen from the first plot that FastGroupIST converges faster than the GroupISt. However, from the perspective of the second plot, we find that the effect of shrinkage of GroupIST is much better than the FastGroupIST. As a result, differenet algorithms have different advantages. What we have to do is choose the appropriate algorithm for the specific situation, and take advantage of it.

##To Dr. Shiyuan He and Dr. Kejun He, 
##thank you so much for your help!
